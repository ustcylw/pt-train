{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refs:\n",
    "\n",
    "https://github.com/michalfaber/dataset_toolkit/blob/main/coco_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# !!! SET THE CORRECT PATH TO YOUR FILES AND FOLDERS !!!\n",
    "\n",
    "cocoRoot = f'/data/ylw/datasets/test/cocos'\n",
    "dataType = \"val2017\"\n",
    "train_annot_path = os.path.join(cocoRoot, f'annotations/instances_{dataType}.json')\n",
    "train_img_path = '/data/ylw/datasets/test/cocos/train2017'\n",
    "\n",
    "val_annot_path = train_annot_path\n",
    "val_img_path = train_img_path\n",
    "\n",
    "train_coco = COCO(train_annot_path)\n",
    "val_coco = COCO(val_annot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(coco):\n",
    "    ids = list(coco.imgs.keys())\n",
    "    for i, img_id in enumerate(ids):\n",
    "        img_meta = coco.imgs[img_id]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        img_file_name = img_meta['file_name']\n",
    "        w = img_meta['width']\n",
    "        h = img_meta['height']\n",
    "        \n",
    "        yield [img_id, img_file_name, w, h, anns]\n",
    "        \n",
    "def convert_to_df(coco):\n",
    "    images_data = []\n",
    "    persons_data = []\n",
    "    \n",
    "    for img_id, img_fname, w, h, meta in get_meta(coco):\n",
    "        images_data.append({\n",
    "            'image_id': int(img_id),\n",
    "            'path': img_fname,\n",
    "            'width': int(w),\n",
    "            'height': int(h)\n",
    "        })\n",
    "        for m in meta: \n",
    "            persons_data.append({\n",
    "                'image_id': m['image_id'],\n",
    "                'is_crowd': m['iscrowd'],\n",
    "                'bbox': m['bbox'],\n",
    "                'area': m['area'],\n",
    "                'num_keypoints': m['num_keypoints'],            \n",
    "                'keypoints': m['keypoints'],            \n",
    "            })\n",
    "    \n",
    "    images_df = pd.DataFrame(images_data) \n",
    "    images_df.set_index('image_id', inplace=True)\n",
    "\n",
    "    persons_df = pd.DataFrame(persons_data) \n",
    "    persons_df.set_index('image_id', inplace=True)\n",
    "    \n",
    "    return images_df, persons_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert COCO dataset format to DataFrames to simplify the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes images and annotations for training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df, persons_df = convert_to_df(train_coco)     \n",
    "train_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)\n",
    "train_coco_df['source'] = 0\n",
    "train_coco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes images and annotations for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df, persons_df = convert_to_df(val_coco)      \n",
    "val_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)\n",
    "val_coco_df['source'] = 1\n",
    "val_coco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine traininng set ('source' = 0) and validation set ('source' = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df = pd.concat([train_coco_df, val_coco_df], ignore_index=True)\n",
    "coco_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how many people are visible in a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of annotations per image\n",
    "\n",
    "annotated_persons_df = coco_df[(coco_df['is_crowd'] == 0)]\n",
    "crowd_df = coco_df[coco_df['is_crowd'] == 1]\n",
    "\n",
    "print(\"Number of people in total: \" + str(len(annotated_persons_df)))\n",
    "print(\"Number of crowd annotations: \" + str(len(crowd_df)))\n",
    "\n",
    "persons_in_img_df = pd.DataFrame({\n",
    "    'cnt': annotated_persons_df[['path','source']].value_counts()\n",
    "})\n",
    "persons_in_img_df.reset_index(level=[0,1], inplace=True) \n",
    "\n",
    "# group by counter so we will get the dataframe with number of annotated people\n",
    "# in a single image\n",
    "\n",
    "persons_in_img_cnt_df = persons_in_img_df.groupby(['cnt']).count()\n",
    "\n",
    "# extract arrays\n",
    "\n",
    "x_occurences = persons_in_img_cnt_df.index.values\n",
    "y_images = persons_in_img_cnt_df['path'].values\n",
    "\n",
    "# plot\n",
    "f = plt.figure(figsize=(14, 8))\n",
    "plt.bar(x_occurences, y_images)\n",
    "plt.title('People on a single image ')\n",
    "plt.xticks(x_occurences, x_occurences)\n",
    "plt.xlabel('Number of people in a single image')\n",
    "plt.ylabel('Number of images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_path(df, row = 0):\n",
    "    path = df['path'].values[row]\n",
    "    source = df['source'].values[row]\n",
    "    if source == 0:\n",
    "        return os.path.join(train_img_path, path)\n",
    "    else:\n",
    "        return os.path.join(val_img_path, path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a few examples of images with 13 annotated persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = persons_in_img_df[(persons_in_img_df['cnt'] == 13) & (persons_in_img_df['source'] == 1)]\n",
    "\n",
    "# take only 9 images -> grid 3x3\n",
    "\n",
    "subset_df = subset_df[:9]\n",
    "\n",
    "# read images\n",
    "\n",
    "full_paths = [get_full_path(subset_df, row = r) for r in range(len(subset_df))]\n",
    "imgs = [mpimg.imread(path) for path in full_paths]\n",
    "\n",
    "# plot\n",
    "\n",
    "_, axs = plt.subplots(3, 3, figsize=(14, 8))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image showing the largest number of annotated people = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = persons_in_img_df[(persons_in_img_df['cnt'] == 19)]\n",
    "\n",
    "path = get_full_path(subset_df)\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)\n",
    "\n",
    "path = subset_df.loc[0, 'path']\n",
    "source = subset_df.loc[0, 'source']\n",
    "subset_full = coco_df[(coco_df['path'] == path) & (coco_df['source'] == source) & (coco_df['is_crowd'] == 0)]\n",
    "bbox = np.array(subset_full['bbox'].values.tolist())\n",
    "for r in bbox:\n",
    "    x = [r[0], r[0] + r[2], r[0] + r[2], r[0], r[0]]\n",
    "    y = [r[1], r[1], r[1] + r[3], r[1] + r[3], r[1]]\n",
    "    plt.plot(x, y, color ='tab:red')      \n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists annotations that are very small, possible without any keypoints.\n",
    "\n",
    "Show statistics of images with persons having some keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_persons_nokp_df = coco_df[(coco_df['is_crowd'] == 0) & (coco_df['num_keypoints'] == 0)]\n",
    "annotated_persons_kp_df = coco_df[(coco_df['is_crowd'] == 0) & (coco_df['num_keypoints'] > 0)]\n",
    "\n",
    "print(\"Number of people (with keypoints) in total: \" + str(len(annotated_persons_kp_df)))\n",
    "print(\"Number of people without any keypoints in total: \" + str(len(annotated_persons_nokp_df)))\n",
    "\n",
    "persons_in_img_kp_df = pd.DataFrame({\n",
    "    'cnt': annotated_persons_kp_df[['path','source']].value_counts()\n",
    "})\n",
    "persons_in_img_kp_df.reset_index(level=[0,1], inplace=True) \n",
    "persons_in_img_cnt_df = persons_in_img_kp_df.groupby(['cnt']).count()\n",
    "x_occurences_kp = persons_in_img_cnt_df.index.values\n",
    "y_images_kp = persons_in_img_cnt_df['path'].values\n",
    "\n",
    "f = plt.figure(figsize=(14, 8))\n",
    "width = 0.4\n",
    "plt.bar(x_occurences_kp, y_images_kp, width=width, label='with keypoints')\n",
    "plt.bar(x_occurences + width, y_images, width=width, label='no keypoints')\n",
    "\n",
    "plt.title('People on a single image ')\n",
    "plt.xticks(x_occurences + width/2, x_occurences)\n",
    "plt.xlabel('Number of people in a single image')\n",
    "plt.ylabel('Number of images')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a few examples of images with 13 annotated persons having at least 1 keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = persons_in_img_kp_df[(persons_in_img_kp_df['cnt'] == 13) & (persons_in_img_kp_df['source'] == 1)]\n",
    "\n",
    "# take only 9 images -> grid 3x3\n",
    "\n",
    "subset_df = subset_df[:9]\n",
    "\n",
    "# read images\n",
    "\n",
    "full_paths = [get_full_path(subset_df, row = r) for r in range(len(subset_df))]\n",
    "imgs = [mpimg.imread(path) for path in full_paths]\n",
    "\n",
    "# plot\n",
    "\n",
    "_, axs = plt.subplots(3, 3, figsize=(14, 8))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extra attributes to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    " \n",
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_keypoints, w_ix, h_ix, bbox_ix, kp_ix):\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.w_ix = w_ix\n",
    "        self.h_ix = h_ix\n",
    "        self.bbox_ix = bbox_ix\n",
    "        self.kp_ix = kp_ix\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do    \n",
    "    \n",
    "    def transform(self, X): \n",
    "        \n",
    "        # retrieve specific columns\n",
    "        \n",
    "        w = X[:, self.w_ix] \n",
    "        h = X[:, self.h_ix]\n",
    "        bbox = np.array(X[:, self.bbox_ix].tolist())\n",
    "        keypoints = np.array(X[:, self.kp_ix].tolist())\n",
    "        \n",
    "        # calculate scale factors for bounding boxes\n",
    "        \n",
    "        scale_x = bbox[:,2] / w   \n",
    "        scale_y = bbox[:,3] / h           \n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        # categorize scales into 4 buckets S,M,L,XL. scale factor = 0.4 means that height of a bounding box \n",
    "        # takes 40% of a total height of an image and will be put into the bucket 'S'\n",
    "        scale_cat = pd.cut(scale_y,\n",
    "                                bins=[0., 0.4, 0.6, 0.8, np.inf],\n",
    "                                labels=['S', 'M', 'L', 'XL']) # 0-0.4  0.4-0.6  0.6-0.8  0.8-1\n",
    "                    \n",
    "        return np.c_[X, scale_x, scale_y, scale_cat, aspect_ratio, keypoints] \n",
    "\n",
    "# get number of keypoints and column indexes    \n",
    "\n",
    "num_keypoints = 17 \n",
    "w_ix = coco_df.columns.get_loc('width') \n",
    "h_ix = coco_df.columns.get_loc('height') \n",
    "bbox_ix = coco_df.columns.get_loc('bbox') \n",
    "kp_ix = coco_df.columns.get_loc('keypoints')\n",
    "\n",
    "# transformer object that is used to add new columns\n",
    "\n",
    "attr_adder = AttributesAdder(\n",
    "    num_keypoints=num_keypoints,\n",
    "    w_ix = w_ix, \n",
    "    h_ix = h_ix, \n",
    "    bbox_ix = bbox_ix, \n",
    "    kp_ix = kp_ix)\n",
    "coco_extra_attribs = attr_adder.transform(coco_df.values)\n",
    "\n",
    "# determine column nmaes for keypoints. In coco dataset each keypoint is represented by a triple x,y,v - coordinates x,y\n",
    "# and visibility flag. Each value is extracted to a separate column in dataframe like x1,x2,v1,... etc\n",
    "keypoints_cols = [['x'+str(idx), 'y'+str(idx), 'v'+str(idx)] for idx, k in enumerate(range(num_keypoints))]\n",
    "keypoints_cols = np.concatenate(keypoints_cols).tolist()\n",
    "\n",
    "# crate a new richer dataframe\n",
    "\n",
    "coco_extra_attribs_df = pd.DataFrame(\n",
    "    coco_extra_attribs,\n",
    "    columns=list(coco_df.columns)+[\"scale_x\", \"scale_y\", \"scale_cat\", \"aspect_ratio\"] + keypoints_cols,\n",
    "    index=coco_df.index)\n",
    "\n",
    "coco_extra_attribs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where are the noses in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only horizontal images to normalize keypoints coordinates\n",
    "horiz_imgs_df = coco_extra_attribs_df[coco_extra_attribs_df['aspect_ratio'] >= 1.]\n",
    "\n",
    "# get the mean width and height - used to scale keypoint coordinates\n",
    "\n",
    "avg_w = int(horiz_imgs_df['width'].mean())\n",
    "avg_h = int(horiz_imgs_df['height'].mean())\n",
    "\n",
    "# indexes of required columns\n",
    "\n",
    "w_ix = horiz_imgs_df.columns.get_loc('width') \n",
    "h_ix = horiz_imgs_df.columns.get_loc('height') \n",
    "x1_ix = horiz_imgs_df.columns.get_loc('x0') # x coord of a nose is the column 'x0'\n",
    "y1_ix = horiz_imgs_df.columns.get_loc('y0') # y coord of a nose is the column 'y0'\n",
    "v1_ix = horiz_imgs_df.columns.get_loc('v0') \n",
    "\n",
    "print (\"avg width \" + str(avg_w))\n",
    "print (\"avg height \" + str(avg_h))\n",
    "\n",
    "class NoseAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, avg_w, avg_h, w_ix, h_ix, x1_ix, y1_ix, v1_ix):\n",
    "        self.avg_w = avg_w\n",
    "        self.avg_h = avg_h\n",
    "        self.w_ix = w_ix \n",
    "        self.h_ix = h_ix \n",
    "        self.x1_ix = x1_ix\n",
    "        self.y1_ix = y1_ix\n",
    "        self.v1_ix = v1_ix\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        w = X[:, self.w_ix] \n",
    "        h = X[:, self.h_ix]         \n",
    "        x1 = X[:, self.x1_ix] \n",
    "        y1 = X[:, self.y1_ix]\n",
    "\n",
    "        # normalize nose coords to the given global width, height\n",
    "        \n",
    "        scale_x = self.avg_w / w\n",
    "        scale_y = self.avg_h / h                \n",
    "        nose_x = x1 * scale_x\n",
    "        nose_y = y1 * scale_y\n",
    "                            \n",
    "        return np.c_[X, nose_x, nose_y]\n",
    "\n",
    "# transformer object that is used to add normalized nose coordinates columns\n",
    "\n",
    "attr_adder = NoseAttributesAdder(\n",
    "    avg_w = avg_w, \n",
    "    avg_h = avg_h,\n",
    "    w_ix = w_ix, \n",
    "    h_ix = h_ix, \n",
    "    x1_ix = x1_ix, \n",
    "    y1_ix = y1_ix, \n",
    "    v1_ix = v1_ix\n",
    ")\n",
    "coco_noses = attr_adder.transform(horiz_imgs_df.values)\n",
    "\n",
    "# crate dataframe with new normalized coordinates\n",
    "\n",
    "coco_noses_df = pd.DataFrame(\n",
    "    coco_noses,\n",
    "    columns=list(horiz_imgs_df.columns) + [\"normalized_nose_x\", \"normalized_nose_y\"],\n",
    "    index=horiz_imgs_df.index)\n",
    "\n",
    "# get only subset of columns\n",
    "\n",
    "coco_noses_df = coco_noses_df[[\"path\", \"source\", \"x0\", \"y0\", \"v0\", \"normalized_nose_x\", \"normalized_nose_y\"]]\n",
    "\n",
    "# filtering - only visible noses\n",
    "\n",
    "coco_noses_df = coco_noses_df[coco_noses_df[\"v0\"] == 2]\n",
    "coco_noses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_noses_df.plot(kind=\"scatter\", x=\"normalized_nose_x\", y=\"normalized_nose_y\", alpha=0.3).invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_noses_df = coco_noses_df[coco_noses_df['normalized_nose_y'] > 430 ]\n",
    "low_noses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '000000289222.jpg'\n",
    "source = 1\n",
    "selected = low_noses_df[(low_noses_df['path'] == path) & (low_noses_df['source'] == source)]\n",
    "\n",
    "full_path = get_full_path(selected)\n",
    "img = mpimg.imread(full_path)\n",
    "plt.imshow(img)\n",
    "plt.plot(selected['x0'], selected['y0'], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_images = coco_extra_attribs_df['num_keypoints'].value_counts()\n",
    "x_keypoints = y_images.index.values\n",
    "\n",
    "# plot\n",
    "\n",
    "plt.figsize=(10,5)\n",
    "plt.bar(x_keypoints, y_images)\n",
    "plt.title('Histogram of keypoints')\n",
    "plt.xticks(x_keypoints)\n",
    "plt.xlabel('Number of keypoints')\n",
    "plt.ylabel('Number of bboxes')\n",
    "plt.show()\n",
    "\n",
    "# percentage of images (column) with a number of keypoints (rows)\n",
    "\n",
    "kp_df = pd.DataFrame({\n",
    "    \"Num keypoints %\": coco_extra_attribs_df[\"num_keypoints\"].value_counts() / len(coco_extra_attribs_df)\n",
    "}).sort_index()\n",
    "kp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_extra_attribs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_df = coco_extra_attribs_df[coco_extra_attribs_df['num_keypoints'] > 0]\n",
    "persons_df['scale_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_props_df = pd.DataFrame({\n",
    "    \"Scales %\": persons_df[\"scale_cat\"].value_counts() / len(persons_df)\n",
    "})\n",
    "scales_props_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is COCO train and validation data stratified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strata: scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_df = coco_extra_attribs_df[coco_extra_attribs_df['num_keypoints'] > 0]\n",
    "train_df = persons_df[persons_df['source'] == 0]\n",
    "val_df = persons_df[persons_df['source'] == 1]\n",
    "\n",
    "scales_props_df = pd.DataFrame({\n",
    "    \"Scales in train set %\": train_df[\"scale_cat\"].value_counts() / len(train_df),\n",
    "    \"Scales in val set %\": val_df[\"scale_cat\"].value_counts() / len(val_df)\n",
    "})\n",
    "scales_props_df[\"Diff 100%\"] = 100 * np.absolute(scales_props_df[\"Scales in train set %\"] - scales_props_df[\"Scales in val set %\"])\n",
    "scales_props_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strata: number of keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = coco_extra_attribs_df[coco_extra_attribs_df['source'] == 0]\n",
    "val_df = coco_extra_attribs_df[coco_extra_attribs_df['source'] == 1]\n",
    "\n",
    "kp_props_df = pd.DataFrame({\n",
    "    \"Num keypoints in train set %\": train_df[\"num_keypoints\"].value_counts() / len(train_df),\n",
    "    \"Num keypoints in val set %\": val_df[\"num_keypoints\"].value_counts() / len(val_df)\n",
    "}).sort_index()\n",
    "kp_props_df[\"Diff 100%\"] = 100 * np.absolute(kp_props_df[\"Num keypoints in train set %\"] - kp_props_df[\"Num keypoints in val set %\"])\n",
    "kp_props_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('train')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d740c7646e63427e6191f8164d347786ef6ce3d268dc00d8555a471847f63fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
